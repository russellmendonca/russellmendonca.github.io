
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Modified from: Jon Barron and Saurabh Gupta. Thanks to Jeff Donahue for email scamble.*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  span.highlight {
  background-color: #ffffd0;
  }
  </style>
  <link rel="shortcut icon" href="https://www.cs.cmu.edu/sites/default/files/favicon_0.ico" type="image/vnd.microsoft.icon">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Russell Mendonca</title>
  <meta name="Russell Mendonca's Homepage" http-equiv="Content-Type" content="Russell Mendonca's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <!-- Start : Google Analytics Code -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-64069893-1', 'auto');
    ga('send', 'pageview');
  </script>
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>

<body>
<table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <!-- <font size="7">Russell Mendonca</font><br> -->
    <pageheading>Russell Mendonca</pageheading><br>
    <!-- <b>email</b>:&nbsp
    <font id="email" style="display:inline;">
      <noscript><i>Please enable Javascript to view</i></noscript>
    </font>
    <script>
    emailScramble = new scrambledString(document.getElementById('email'),
        'emailScramble', 'umepc@aah.tud.skdc',
        [18,13,16,2,12,8,6,3,5,15,4,14,17,11,10,7,1,9]);
    </script> -->
  </p>

  <tr>
    <td width="32%" valign="top"><a href="images/russell_photo_circle.png"><img src="images/russell_photo_circle.png" width="100%" style="border-radius:15px"></a>
    <p align=center>
    <a href="CV_Mendonca.pdf">CV</a> |
    <a href="https://scholar.google.com/citations?user=Uly5spMAAAAJ&hl=en&oi=ao">Google Scholar</a>  |
    <!-- <a href="https://github.com/russellmendonca">Github</a> | -->
    <a href="https://twitter.com/mendonca_rl">Twitter</a> <br/>
    <p align=center style="margin-top:-8px;" ><a href="https://twitter.com/mendonca_rl?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @Mendonca2206</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
    </p>
    </td>
    <td width="68%" valign="top" align="justify">
    <p> I am a fourth year PhD student in the Robotics Institute at Carnegie Mellon University,
        advised by <a href="https://www.cs.cmu.edu/~dpathak/"> Prof. Deepak Pathak, </a>
        interested in working on problems in machine learning, robotics and computer vision
        to enable general purpose agents.

    <p> Previously, I graduated from UC Berkeley in Electrical Engineering and Computer Science,
    and worked on reinforcement learning with Prof. Sergey Levine in the
      <a href="https://bair.berkeley.edu"> Berkeley Artificial Intelligence Lab (BAIR)</a>
    </p>

    <p>
      You can contact me via email at rmendonc -at- andrew dot cmu dot edu
    </p>
   

    </td>
  </tr>
</table>

<hr/>

<!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td>
    <sectionheading>&nbsp;&nbsp;News/Media</sectionheading>
    <ul>
      <li> VRB - media.</li>
      <li> ALAN - media</li>
    </ul>
  </td></tr>
</table>
<hr/> -->



<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Research </sectionheading></td></tr>
  <!-- <p>
    I am broadly interested in creating robust autonomous agents that operate with minimal or no human supervision, in the wild. My research focuses on learning for perception and robot control. Here is some of my work (representative papers are <span class="highlight">highlighted</span>):  
  </p> -->
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
   <p>
    I am broadly interested in building generalist agents that can continually improve in capability. I have studied this for robots that explore to collect their own data, including intrinsic motivation objectives, using human video and world models, and autonomous mobile manipulation systems. Here is some of my work (representative papers are <span class="highlight">highlighted</span>):  </p>
<br/>


<tr bgcolor="#ffffd0">
  <td width="33%" valign="top" align="center"><a href="https://spot-rl-manip.github.io/">
  <video autoplay="" loop="" muted="" src="images/spotRL.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" width="90%"></video>
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://spot-rl-manip.github.io/" id="SWIM">
    <heading>Continuously Improving Mobile Manipulation <br> with Autonomous Real-World RL</heading></a><br>

    Russell Mendonca, Bernadette Bucher, Jiuguang Wang, Deepak Pathak<br>
    <i>In submission</i> &nbsp;<br>
    </p>

    <div class="paper" id="spotrl">
    <a href="https://spot-rl-manip.github.io/">webpage</a> |
    <a href="javascript:toggleblock('spotrl-abs')">abstract</a> 
    <!-- <a shape="rect" href="javascript:togglebib('spotrl')" class="togglebib">bibtex</a> | -->
   
    <p align="justify"> <i style="display: none;" id="spotrl-abs">To build generalist robots capable of executing a wide array of tasks across diverse environments, robots must be endowed with the ability to engage directly with the real world to acquire and refine skills without extensive instrumentation or human supervision. This work presents a fully autonomous real-world reinforcement learning framework for mobile manipulation that can both independently gather data and refine policies through accumulated experience in the real world. It has several key components: 
      1) automated data collection strategies by guiding the exploration of the robot toward object interactions, 
      2) using goal cycles for world RL such that the robot changes goals once it has made sufficient progress, where the different goals serve as resets for one another, 
      3) efficient control by leveraging basic task knowledge present in behavior priors in conjunction with policy learning and 
      4) formulating generic rewards that combine human-interpretable semantic information with low-level, fine-grained state information. 
      We demonstrate our approach on Boston Dynamics Spot robots in continually improving performance on a set of four challenging mobile manipulation tasks and show that this enables competent policy learning, obtaining an average success rate of 80% across tasks, a 3-4 times improvement over existing approaches.
    </i></p>

    <!-- <pre xml:space="preserve" style="display:none;">
  @article{mendonca23swim,
  title={Structured World Models 
  from Human Videos},
  author={Mendonca, Russell and 
  Bahl, Shikhar and Pathak, Deepak},
  journal={RSS},
  year={2023},
}
</pre> -->
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://vader-eccv.github.io/">
  <img src="images/vader.gif" alt="sym" width="80%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:0px solid black">
  
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://vader-eccv.github.io/" id="VADER">
    <heading>Video Diffusion Alignment via Reward Gradients</heading></a><br>

    Mihir Prabhudesai*, Russell Mendonca*, Katerina Fragkiadaki,<br> Deepak Pathak<br>
    <i>In submission</i> &nbsp;<br>
    </p>

    <div class="paper" id="vader">
    <a href="https://vader-eccv.github.io/">webpage</a> |
    <a href="javascript:toggleblock('vader-abs')">abstract</a> 
   
    <p align="justify"> <i style="display: none;" id="vader-abs">We have made significant progress towards building foundational video diffusion models. As these models are trained using large-scale unsupervised data, it has become crucial to adapt these models to specific downstream tasks, such as video-text alignment or ethical video generation. Adapting these models via supervised fine-tuning requires collecting target datasets of videos, which is challenging and tedious. In this work, we instead utilize pre-trained reward models that are learned via preferences on top of powerful discriminative models. These models contain dense gradient information with respect to generated RGB pixels, which is critical to be able to learn efficiently in complex search spaces, such as videos. We show that our approach can enable alignment of video diffusion for aesthetic generations, similarity between text context and video, as well long horizon video generations that are 3X longer than the training sequence length. We show our approach can learn much more efficiently in terms of reward queries and compute than previous gradient-free approaches for video generation.
    </i></p>

    <!-- <pre xml:space="preserve" style="display:none;">
  @article{mendonca23swim,
  title={Structured World Models 
  from Human Videos},
  author={Mendonca, Russell and 
  Bahl, Shikhar and Pathak, Deepak},
  journal={RSS},
  year={2023},
}
</pre> -->
    </div>
  </td>
</tr>




<tr bgcolor="#ffffd0">
  <td width="33%" valign="top" align="center"><a href="https://door-open.github.io/">
  <video autoplay="" loop="" muted="" src="images/door.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" width="90%"></video>
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://door-open.github.io/" id="door-open">
    <heading>Adaptive Mobile Manipulation for Articulated <br> Objects in the Open World </heading></a><br>

    Haoyu Xiong, Russell Mendonca, Kenneth Shaw, Deepak Pathak <br>
    <i>In submission</i> &nbsp;<br>
    </p>

    <div class="paper" id="dooropen">
    <a href="https://door-open.github.io/">webpage</a> |
    <a href="https://arxiv.org/pdf/2401.14403.pdf">pdf</a> |
    <a href="javascript:toggleblock('dooropen-abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('dooropen')" class="togglebib">bibtex</a> |
   
    <p align="justify"> <i style="display: none;" id="dooropen-abs">Deploying robots in open-ended unstructured environments such as homes has been a long-standing research problem. However, robots are often studied only in closed-off lab settings, and prior mobile manipulation work is restricted to pick-move-place, which is arguably just the tip of the iceberg in this area. In this paper, we introduce Open-World Mobile Manipulation System, a full-stack approach to tackle realistic articulated object operation, e.g. real-world doors, cabinets, drawers, and refrigerators in open-ended unstructured environments. The robot utilizes an adaptive learning framework to initially learns from a small set of data through behavior cloning, followed by learning from online practice on novel objects that fall outside the training distribution. We also develop a low-cost mobile manipulation hardware platform capable of safe and autonomous online adaptation in unstructured environments with a cost of around 20,000 USD. In our experiments we utilize 20 articulate objects across 4 buildings in the CMU campus. With less than an hour of online learning for each object, the system is able to increase success rate from 50% of BC pre-training to 95% using online adaptation.</i></p>

    <pre xml:space="preserve" style="display:none;">
  @article{xiong2024adaptive,
    title={Adaptive mobile manipulation 
    for articulated objects in the open world},
    author={Xiong, Haoyu and Mendonca, 
    Russell and Shaw, Kenneth and Pathak, Deepak},
    journal={arXiv preprint arXiv:2401.14403},
    year={2024},
  }
</pre>
    </div>
  </td>
</tr>

<tr>
  <td width="33%" valign="top" align="center"><a href="https://robotics-transformer-x.github.io/">
  <video autoplay="" loop="" muted="" src="images/openX.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" width="90%"></video>
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://robotics-transformer-x.github.io/" id="SWIM">
    <heading>OpenX Embodiment: Robotic Learning Datasets and RT-X Models </heading></a><br>

    Open X-Embodiment Collaboration<br>
    ICRA 2024 &nbsp;<br>
    </p>

    <div class="paper" id="openX">
    <a href="https://robotics-transformer-x.github.io/">webpage</a> |
    <a href="https://arxiv.org/pdf/2310.08864.pdf">pdf</a> |
    <a href="javascript:toggleblock('openX-abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('openX')" class="togglebib">bibtex</a> |
   
    <p align="justify"> <i style="display: none;" id="openX-abs">Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train “generalist” X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms.
      
    </i></p>

    <pre xml:space="preserve" style="display:none;">
      @misc{open_x_embodiment_rt_x_2023,
        title={Open {X-E}mbodiment: Robotic
        Learning Datasets and {RT-X} Models},
        author = {Open X-Embodiment Collaboration },
        howpublished  = {\url{https://arxiv.org/abs/2310.08864}},
        year = {2023},
        }
    </pre>
    </div>
  </td>
</tr>


<tr bgcolor="#ffffd0">
  <td width="33%" valign="top" align="center"><a href="https://human-world-model.github.io/">
  <video autoplay="" loop="" muted="" src="images/swim.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" width="90%"></video>
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://human-world-model.github.io/" id="SWIM">
    <heading>Structured World Models from Human Videos</heading></a><br>

    Russell Mendonca*, Shikhar Bahl*, Deepak Pathak<br>
    RSS 2023 &nbsp;<br>
    </p>

    <div class="paper" id="swim">
    <a href="https://human-world-model.github.io/">webpage</a> |
    <a href="https://www.roboticsproceedings.org/rss19/p012.pdf">pdf</a> |
    <a href="javascript:toggleblock('swim-abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('swim')" class="togglebib">bibtex</a> |
   
    <p align="justify"> <i style="display: none;" id="swim-abs">We tackle the problem of learning complex, general behaviors directly in the real world. We propose an approach for robots to efficiently learn manipulation skills using only a handful of real-world interaction trajectories from many different settings. Inspired by the success of learning from large-scale datasets in the fields of computer vision and natural language, our belief is that in order to efficiently learn, a robot must be able to leverage internet-scale, human video data. Humans interact with the world in many interesting ways, which can allow a robot to not only build an understanding of useful actions and affordances but also how these actions affect the world for manipulation. Our approach builds a structured, human-centric action space grounded in visual affordances learned from human videos. Further, we train a world model on human videos and fine-tune on a small amount of robot interaction data without any task supervision. We show that this approach of affordance-space world models enables different robots to learn various manipulation skills in complex settings, in under 30 minutes of interaction.</i></p>

    <pre xml:space="preserve" style="display:none;">
  @article{mendonca23swim,
  title={Structured World Models 
  from Human Videos},
  author={Mendonca, Russell and 
  Bahl, Shikhar and Pathak, Deepak},
  journal={RSS},
  year={2023},
}
</pre>
    </div>
  </td>
</tr>


<tr bgcolor="#ffffd0">
  <td width="33%" valign="top" align="center"><a href="https://robo-affordances.github.io/">
  <video autoplay="" loop="" muted="" src="images/vrb_small.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" width="90%"></video>
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://robo-affordances.github.io/" id="SWIM">
    <heading>Affordances from Human Videos as a Versatile  <br> Representation for Robotics</heading></a><br>
    Shikhar Bahl*, Russell Mendonca*, Lili Chen, Unnat Jain, <br> Deepak Pathak<br>
    CVPR 2023
    </p>

    <div class="paper" id="vrb">
    <a href="https://robo-affordances.github.io/">webpage</a> |
    <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bahl_Affordances_From_Human_Videos_as_a_Versatile_Representation_for_Robotics_CVPR_2023_paper.pdf">pdf</a> |
    <a href="javascript:toggleblock('vrb-abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('vrb')" class="togglebib">bibtex</a> |
   
    <p align="justify"> <i style="display: none;" id="vrb-abs">Building a robot that can understand and learn to interact by watching humans has inspired several vision problems. However, despite some successful results on static datasets, it remains unclear how current models can be used on a robot directly. In this paper, we aim to bridge this gap by leveraging videos of human interactions in an environment centric manner. Utilizing internet videos of human behavior, we train a visual affordance model that estimates where and how in the scene a human is likely to interact. The structure of these behavioral affordances directly enables the robot to perform many complex tasks. We show how to seamlessly integrate our affordance model with four robot learning paradigms including offline imitation learning, exploration, goal-conditioned learning, and action parameterization for reinforcement learning. We show the efficacy of our approach, which we call Vision-Robotics Bridge (VRB) as we aim to seamlessly integrate computer vision techniques with robotic manipulation, across 4 real world environments, over 10 different tasks, and 2 robotic platforms operating in the wild.</i></p>
          
    <pre xml:space="preserve" style="display:none;">
      @article{bahl2023affordances,
      title={Affordances from Human Videos 
      as a Versatile Representation 
      for Robotics},
      author={Bahl, Shikhar and Mendonca, 
      Russell and Chen, Lili and Jain, 
      Unnat and Pathak, Deepak},
      journal={CVPR},
      year={2023}
      }
    </pre>
    </div>
  </td>
</tr>

<tr>
  <td width="33%" valign="top" align="center"><a href="https://sear-rl.github.io/">
  <img src="images/sear.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:0px solid black">
  
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://sear-rl.github.io/" id="SEAR">
    <heading>Efficient RL via Disentangled Environment and <br> Agent Representations</heading></a><br>
    Kevin Gmelin*, Shikhar Bahl*, Russell Mendonca, Deepak Pathak <br> 
    ICML 2023
    </p>

    <div class="paper" id="sear">
    <a href="https://sear-rl.github.io/">webpage</a> |
    <a href="https://openreview.net/pdf?id=kWS8mpioS9">pdf</a> |
    <a href="javascript:toggleblock('sear-abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('sear')" class="togglebib">bibtex</a> |
   
    <p align="justify"> <i style="display: none;" id="sear-abs">Agents that are aware of the separation between the environments and themselves can leverage this understanding to form effective representations of visual input. We propose an approach for learning such structured representations for RL algorithms, using visual knowledge of the agent, which is often inexpensive to obtain, such as its shape or mask. This is incorporated into the RL objective using a simple auxiliary loss. We show that our method, SEAR (Structured Environment-Agent Representations), outperforms state-of-the-art model-free approaches over 18 different challenging visual simulation environments spanning 5 different robots.</i></p>
          
    <pre xml:space="preserve" style="display:none;">
      @article{Gmelin2023sear,
      title={Efficient RL via Disentangled 
      Environment and Agent Representations},
      author={Gmelin, Kevin and Bahl, Shikhar
      and Mendonca, Russell and Pathak, Deepak},
      journal={ICML},
      year={2023}
      }
    </pre>
    </div>
  </td>
</tr>

<tr bgcolor="#ffffd0">
  <td width="33%" valign="top" align="center"><a href="https://robo-explorer.github.io/">
  <video autoplay="" loop="" muted="" src="images/alan.mp4" alt="sym" style="border: 1px solid #bbb;padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" width="90%"></video>
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://robo-explorer.github.io/" id="ALAN">
    <heading>ALAN : Autonomously Exploring Robotic Agents
      in the Real World</heading></a>
    <br>Russell Mendonca, Shikhar Bahl, Deepak Pathak<br>
    ICRA 2023 
    </p>

    <div class="paper" id="alan">
    <a href="https://robo-explorer.github.io/">webpage</a> |
    <a href="https://arxiv.org/pdf/2302.06604.pdf">pdf</a> |
    <a href="javascript:toggleblock('alan-abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('alan')" class="togglebib">bibtex</a> |
   

    <p align="justify"> <i style="display: none;" id="alan-abs">Robotic agents that operate autonomously in the real world need to continuously explore their environment and learn from the data collected, with minimal human supervision. While it is possible to build agents that can learn in such a manner without supervision, current methods struggle to scale to the real world. Thus, we propose ALAN, an autonomously exploring robotic agent, that can perform many tasks in the real world with little training and interaction time. This is enabled by measuring environment change, which reflects object movement and ignores changes in the robot position. We use this metric directly as an environment-centric signal, and also maximize the uncertainty of predicted environment change, which provides agent-centric exploration signal. We evaluate our approach on two different real-world play kitchen settings, enabling a robot to efficiently explore and discover manipulation skills, and perform tasks specified via goal images.</i></p>

<pre xml:space="preserve" style="display:none;">
  @article{mendonca2023alan,
    author = {Mendonca, Russell and
    Bahl, Shikhar and
    Pathak, Deepak},
    title  = {ALAN : Autonomously Exploring 
    Robotic Agents in the Real World},
    journal= {ICRA},
    year   = {2023}
  }
</pre>
    </div>
  </td>
</tr>


<tr bgcolor="#ffffd0">
  <td width="33%" valign="top" align="center"><a href="https://orybkin.github.io/lexa/">
  <img src="images/lexa_grid.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:0px solid black">
  <!-- <video autoplay loop muted src="images/lexa.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video> -->
  </a></td>
  <td width="67%" valign="top">
  <p><a href="https://orybkin.github.io/lexa/" id="LEXA">
  <!-- <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none"> -->
  <heading>Discovering and Achieving Goals via World Models</heading></a><br>
  Russell Mendonca*, Oleh Rybkin*, <br> Kostas Daniilidis, Danijar Hafner, Deepak Pathak<br>
  NeurIPS 2021
  </p>
  
  <div class="paper" id="lexa">
  <a href="https://orybkin.github.io/lexa/">webpage</a> |
  <a href="https://arxiv.org/pdf/2110.09514.pdf">pdf</a> |
  <a href="javascript:toggleblock('lexa_abs')">abstract</a> |
  <a shape="rect" href="javascript:togglebib('lexa')" class="togglebib">bibtex</a> |
  <!-- <a href="https://arxiv.org/abs/2107.04034">arXiv</a> | -->
  <a href="https://github.com/orybkin/lexa">code</a> |
  <a href="https://github.com/orybkin/lexa-benchmark">benchmark</a> |
  <a href="https://www.youtube.com/watch?v=WWHlQbigQp4">talk video</a>
  
  <p align="justify"> <i style="display: none;" id="lexa_abs">How can artificial agents learn to solve wide ranges of tasks in complex visual environments in the absence of external supervision? We decompose this question into two problems, global exploration of the environment and learning to reliably reach situations found during exploration. We introduce the Latent Explorer Achiever (LEXA), a unified solution to these by learning a world model from the high-dimensional image inputs and using it to train an explorer and an achiever policy from imagined trajectories. Unlike prior methods that explore by reaching previously visited states, the explorer plans to discover unseen surprising states through foresight, which are then used as diverse targets for the achiever. After the unsupervised phase, LEXA solves tasks specified as goal images zero-shot without any additional learning. We introduce a challenging benchmark spanning across four standard robotic manipulation and locomotion domains with a total of over 40 test tasks. LEXA substantially outperforms previous approaches to unsupervised goal reaching, achieving goals that require interacting with multiple objects in sequence. Finally, to demonstrate the scalability and generality of LEXA, we train a single general agent across four distinct environments.</i></p>
  
  <pre xml:space="preserve">
  @inproceedings{mendonca2021lexa,
  Author = {Mendonca, Russell and
  Rybkin, Oleh and Daniilidis, Kostas and
  Hafner, Danijar and Pathak, Deepak},
  Title = {Discovering and Achieving
  Goals via World Models},
  Booktitle = {NeurIPS},
  Year = {2021}
  }
  </pre>
  </div>
  </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="http://papers.nips.cc/paper/9160-guided-meta-policy-search.pdf">
    <img src="images/gmps.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:0px solid black">
    <!-- <video autoplay loop muted src="images/lexa.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video> -->
    </a></td>
    <td width="67%" valign="top">
    <p><a href="http://papers.nips.cc/paper/9160-guided-meta-policy-search.pdf" id="LEXA">
    <!-- <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none"> -->
    <heading>Guided Meta-Policy Search</heading></a><br>
    Russell Mendonca, Abhishek Gupta, Rosen Kralev, <br> Pieter Abbeel, Sergey Levine, Chelsea Finn <br>
    NeurIPS 2019 (spotlight)
    </p>
    
    <div class="paper" id="gmps">
    <a href="https://sites.google.com/berkeley.edu/guided-metapolicy-search/home">webpage</a> |
    <a href="http://papers.nips.cc/paper/9160-guided-meta-policy-search.pdf">pdf</a> |
    <a href="javascript:toggleblock('gmps_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('gmps')" class="togglebib">bibtex</a> |
    <!-- <a href="https://arxiv.org/abs/2107.04034">arXiv</a> | -->
    <a href="https://github.com/russellmendonca/GMPS">code</a> |
    
    <p align="justify"> <i style="display: none;" id="gmps_abs">We develop an algorithm which can meta-learn in a data efficient manner and also train on raw visual input, by reformulating the meta-learning objective to have imitation learning as a subroutine. We show about an order of magnitude sample efficiency gain on challenging simulation environments, and much more stable learning from high dimensional image observations as compared to prior state of the art methods.</i></p>
      
    <pre xml:space="preserve">
    @inproceedings{mendonca2019gmps,
    Author = {Mendonca, Russell and
    Gupta, Abhishek and Kralev, Rosen and
    Abbeel, Pieter and Levine, 
    Sergey and Finn, Chelsea},
    Title = {Guided Meta-Policy Search},
    Booktitle = {NeurIPS},
    Year = {2019}
    }
    </pre>
    </div>
    </td>
    </tr>

    <tr>
      <td width="33%" valign="top" align="center"><a href="http://papers.nips.cc/paper/7776-meta-reinforcement-learning-of-structured-exploration-strategies.pdf">
      <img src="images/maesn.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:0px solid black">
      <!-- <video autoplay loop muted src="images/lexa.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video> -->
      </a></td>
      <td width="67%" valign="top">
      <p><a href="http://papers.nips.cc/paper/7776-meta-reinforcement-learning-of-structured-exploration-strategies.pdf" id="MAESN">
      <!-- <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none"> -->
      <heading>Meta-Reinforcement Learning of Structured Exploration Strategies</heading></a><br>
      Abhishek Gupta, Russell Mendonca, YuXuan Liu, <br> Pieter Abbeel, Sergey Levine <br>
      NeurIPS 2018 (spotlight)
      </p>
      
      <div class="paper" id="maesn">
      <a href="data/NIPS_MAESN_final.pptx">slides</a> |
      <a href="http://papers.nips.cc/paper/7776-meta-reinforcement-learning-of-structured-exploration-strategies.pdf">pdf</a> |
      <a href="javascript:toggleblock('maesn_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('maesn')" class="togglebib">bibtex</a> |
      <!-- <a href="https://arxiv.org/abs/2107.04034">arXiv</a> | -->
      <a href="https://github.com/russellmendonca/maesn_suite.git">code</a> |
      
      <p align="justify"> <i style="display: none;" id="maesn_abs">We design a meta-learning algorithm that acquires coherent exploration strategies, in addition to adapting
        quickly to new tasks. This enables learning on new tasks with sparse feedback. Given a set of tasks,
        we meta-learn a representation space and then explore in the learnt task space instead of the space of
      random actions, resulting in more meaningful exploratory behavior.</i></p>
        
      <pre xml:space="preserve">
      @inproceedings{gupta2018maesn,
      Author = {Gupta, Abhishek and 
      Mendonca, Russell and Liu, YuXuan and
      Abbeel, Pieter and Levine, Sergey},
      Title = {Meta-Reinforcement Learning 
      of Structured Exploration Strategies},
      Booktitle = {NeurIPS},
      Year = {2018}
      }
      </pre>
      </div>
      </td>
      </tr>
    
      <tr>
        <td width="33%" valign="top" align="center"><a href="https://arxiv.org/pdf/2006.07178.pdf">
        <img src="images/mier.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:0px solid black">
        <!-- <video autoplay loop muted src="images/lexa.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video> -->
        </a></td>
        <td width="67%" valign="top">
        <p><a href="https://arxiv.org/pdf/2006.07178.pdf" id="MIER">
        <!-- <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none"> -->
        <heading>Meta-Reinforcement Learning Robust to Distributional Shift via Model Identification and Experience Relabeling</heading></a><br>
        Russell Mendonca *, Xinyang Geng *, Chelsea Finn, Sergey Levine <br>
        Inductive biases, invariances and generalization in RL <br> ICML Workshop , 2020
        </p>
        
        <div class="paper" id="mier">
        <a href="https://arxiv.org/pdf/2006.07178.pdf">pdf</a> |
        <a href="javascript:toggleblock('mier_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('mier')" class="togglebib">bibtex</a> |
       
        <p align="justify"> <i style="display: none;" id="mier_abs">We develop a meta-learning algorithm that can generalize efficiently to unfamiliar tasks. Recognizing
          that supervised learning is much more effective than RL for extrapolation to out-of-distribution tasks,
          we meta-learn models of state dynamics, for which a natural supervised objective exists. Given a new
          task, we use synthetic data generated from the learned model for continued training, ensuring that
          extrapolation is highly data-efficient.</i></p>
          
        <pre xml:space="preserve">
          @misc{mendonca2020metareinforcement,
            title={Meta-Reinforcement Learning 
            Robust to Distributional Shift 
            via Model Identification 
            and Experience Relabeling}, 
            author={Russell Mendonca and Xinyang 
            Geng and Chelsea Finn and Sergey Levine},
            year={2020},
            eprint={2006.07178},
            archivePrefix={arXiv},
            primaryClass={cs.LG}
      }

        </pre>
        </div>
        </td>
        </tr>
    

</table>

<!--
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Teaching</sectionheading></td></tr>
</table>
<table width="100%" align="top" border="0" cellpadding="20">
  <tr>
    <td width="32%"><img src="images/cs189.jpg" alt="pacman" width="100%" style="border-radius:15px"></td>
    <td width="68%" valign="top">
      <p>
        <a href="http://www-inst.eecs.berkeley.edu/~cs189/fa15/"><heading>CS189/289: Introduction to Machine Learning - Fall '15 (GSI) </heading></a><br>
        <strong>Instructor</strong>: Prof. Alexei A. Efros and Dr. Isabelle Guyon<br>
      </p>
      <p>
        <a href="http://www-inst.eecs.berkeley.edu/~cs280/sp16"><heading>CS280: Computer Vision - Spring '16 (GSI)</heading></a><br>
        <strong>Instructor</strong>: Prof. Trevor Darrell and Prof. Alexei A. Efros<br>
      </p>
    </td>
  </tr>
</table> -->

<!-- <hr/>

<table width="100%" align="center" border="0" cellpadding="10">
  <tr><td>
    <sectionheading>&nbsp;&nbsp;Selected Awards</sectionheading>
    <ul>
    <li> Google Faculty Research Award (2020)</li>
    <li> Facebook Graduate Fellowship (2018-2020)</li>
    <li> Nvidia Graduate Fellowship (2017-2018)</li>
    <li> Snapchat Inc. Graduate Fellowship (2017)</li>
    <li> Gold Medal in Computer Science at IIT Kanpur (2014)</li>
    <li> Best Undergraduate Thesis Award at IIT Kanpur (2014)</li>
    </ul>
  </td></tr>
</table> -->

<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tr><td><br><p align="right"><font size="1.5">
    Modified version of template from <a href="http://www.cs.berkeley.edu/~barron/">here</a>
    </font></p></td></tr>
</table>

  </td></tr>
</table>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('jpm15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('fg15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('wacv15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclr15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cvpr15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iccv15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('jmlr16_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cvpr16_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclr16_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cvpr17_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('icml17_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('nips17_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclr18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('icml18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cvprw18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclr19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('assemblies19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('sgm20_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('compgan18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('icml19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('phd19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('neurips19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('plan2explore_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('icml20_smp_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('uai20_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('neurips20_ndp_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclr21_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('worldsheet_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('flavr_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('icra21_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('ral21_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('keypoint3D_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('spt_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('hndp_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('rma_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('energyloco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('lexa_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('raps_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('lff_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('envexplore_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('rtk_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('whirl_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('tars_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('clear_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('rb2_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('navloco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('gptplanner_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('geometrydex_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('revolver_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('eccv22_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('clipICML22_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('arma_abs');
</script>
</body>

</html>
